{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"./nips/papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>454</td>\n",
       "      <td>1991</td>\n",
       "      <td>Gradient Descent: Second Order Momentum and Sa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gradient Descent: Second-Order Momentum \\n\\nan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>424</td>\n",
       "      <td>1990</td>\n",
       "      <td>Stochastic Neurodynamics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stochastic Neurodynamics \\n\\nJ.D. Cowan \\n\\nDe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>1755</td>\n",
       "      <td>1999</td>\n",
       "      <td>Optimal Kernel Shapes for Local Linear Regression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Optimal Kernel  Shapes for  Local  Linear \\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6294</th>\n",
       "      <td>1140</td>\n",
       "      <td>2016</td>\n",
       "      <td>Mixed Linear Regression with Multiple Components</td>\n",
       "      <td>In this paper, we study the mixed linear regre...</td>\n",
       "      <td>Mixed Linear Regression with Multiple Componen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5418</th>\n",
       "      <td>1443</td>\n",
       "      <td>2014</td>\n",
       "      <td>Multi-Step Stochastic ADMM in High Dimensions:...</td>\n",
       "      <td>In this paper, we consider a multi-step versio...</td>\n",
       "      <td>Multi-Step Stochastic ADMM in High Dimensions:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source_id  year                                              title  \\\n",
       "553         454  1991  Gradient Descent: Second Order Momentum and Sa...   \n",
       "324         424  1990                           Stochastic Neurodynamics   \n",
       "1729       1755  1999  Optimal Kernel Shapes for Local Linear Regression   \n",
       "6294       1140  2016   Mixed Linear Regression with Multiple Components   \n",
       "5418       1443  2014  Multi-Step Stochastic ADMM in High Dimensions:...   \n",
       "\n",
       "                                               abstract  \\\n",
       "553                                                 NaN   \n",
       "324                                                 NaN   \n",
       "1729                                                NaN   \n",
       "6294  In this paper, we study the mixed linear regre...   \n",
       "5418  In this paper, we consider a multi-step versio...   \n",
       "\n",
       "                                              full_text  \n",
       "553   Gradient Descent: Second-Order Momentum \\n\\nan...  \n",
       "324   Stochastic Neurodynamics \\n\\nJ.D. Cowan \\n\\nDe...  \n",
       "1729  Optimal Kernel  Shapes for  Local  Linear \\n\\n...  \n",
       "6294  Mixed Linear Regression with Multiple Componen...  \n",
       "5418  Multi-Step Stochastic ADMM in High Dimensions:...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9680, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={\"full_text\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_id       0\n",
       "year            0\n",
       "title           0\n",
       "abstract     3319\n",
       "text            3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minlingzhou/miniconda3/envs/my-env/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/minlingzhou/miniconda3/envs/my-env/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def tokenize_text(data, tokenizer):\n",
    "    \"\"\"Tokenize text using tokenizer and return tokens.\"\"\"\n",
    "    content = data[\"title\"]\n",
    "    content += data[\"abstract\"] if data[\"abstract\"] else \"\"\n",
    "    content += data[\"text\"] if data[\"text\"] else \"\"\n",
    "    ids = tokenizer.encode(\n",
    "        content,\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    return {\"ids\": ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bd255359204ebd92c71ad37a9f4cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize_text, fn_kwargs={\"tokenizer\": tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([  101, 20831,  6431, 23077,  2487, 20831,  6431, 23077,  8038,  8043,\n",
       "          1055,  1012,  8273,  1011,  2087, 10354,  2050,  2662,  2820,  1997,\n",
       "          2974, 18880,  1010,  6187, 19989, 17788, 10061,  2129,  2515,  1996,\n",
       "         20831,  1997,  1037, 15756,  2897,  1006,  2193,  1997, 19962,  9331,\n",
       "          8583,  2566, 11265, 21017,  1007, 14396,  2000,  1996, 11619,  1997,\n",
       "          1996,  3471,  2009,  2064,  5047,  1006,  7594,  2011,  1996, 23077,\n",
       "          1007,  1029, 11991,  3399,  2052,  6592,  2053,  7189,  2012,  2035,\n",
       "          1010,  2144,  2035, 22017, 20898,  4972,  2064,  2022,  7528,  2478,\n",
       "          1037,  4984,  2007,  2200,  2659, 20831,  1006,  1041,  1012,  1043,\n",
       "          1012,  1010,  2478,  2048,  1011,  7953, 16660,  2094,  6733,  1007,\n",
       "          1012,  2174,  1010,  2005,  1037,  2897,  2008, 10229,  1037,  3291,\n",
       "          2013,  4973,  2478,  1037,  2334,  4083,  3627,  1010,  2057,  6011,\n",
       "          2008,  1996, 23077,  1997,  1996,  3291,  4150,  1037,  2896,  5391,\n",
       "          2005,  1996, 20831,  1997,  1996,  2897,  1012,  4955,  1996,  2087,\n",
       "         20852,  3444,  1997, 15756,  6125,  2003,  2037,  3754,  2000, 11867,\n",
       "          2239,  1006, 28744,  1024, 19410,  1007,  9092, 14769,  2135,  4553,\n",
       "          1996,  9059,  3853,  2013,  1005,  2731,  1005,  8168,  1010,  1045,\n",
       "          1012,  1041,  1012,  1010,  2037,  3754,  2000,  2565,  3209,  1012,\n",
       "          4415,  1010,  1037,  2445, 15756,  2897,  3685,  2074,  4553,  2151,\n",
       "          3853,  1010,  2045,  2442,  2022,  2070,  9259,  2006,  2029,  6125,\n",
       "          2064,  4553,  2029,  4972,  1012,  2028,  5793, 16840,  1010,  2029,\n",
       "          2003,  2981,  1997,  1996,  4083,  7814,  1010,  2003,  2008,  1996,\n",
       "          2897,  2442,  2022,  2502,  2438,  2000,  8752,  1996,  4984,  3375,\n",
       "          1006, 28744,  1024, 19410,  1007,  2009,  2100,  1997,  1996,  3853,\n",
       "          2009,  2097,  2776, 26633,  1012,  2024,  2045,  9259,  2008, 13368,\n",
       "          6414,  2013,  1996,  2755,  2008,  1996,  2897,  2003,  3517,  2000,\n",
       "          4553,  1996,  3853,  1010,  2738,  2084,  2108, 24680,  2881,  2005,\n",
       "          1996,  3853,  1029,  2023,  3259,  4311,  1037, 16840,  1997,  2023,\n",
       "          2785,  1012,  1996,  2765, 17607,  2015,  1037,  2896,  5391,  2006,\n",
       "          1996, 20831,  1997,  1996,  2897,  1006, 16371,  2213,  1006, 28744,\n",
       "          1024, 19410,  1007,  2022,  2099,  1997, 19962,  9331,  8583,  2566,\n",
       "         11265, 21017,  1007,  1012,  2023,  2896,  5391,  2064,  2069,  2022,\n",
       "          1037,  9509,  1997,  1996,  4083,  7814,  1010,  2144, 11991,  3399,\n",
       "          3640, 24680,  2881, 13782,  1997,  2659, 20831,  1006,  1041,  1012,\n",
       "          1043,  1012,  1010,  2478,  2069,  2048,  1011,  7953, 16660,  2094,\n",
       "          6733,  1007,  5214,  1997, 17727,  2571,  1006, 28744,  1024, 19410,\n",
       "          1007,  2273,  3436,  2151, 22017, 20898,  3853,  1031,  1015,  1010,\n",
       "          1016,  1033,  1012,  2009,  2036,  4076,  2008,  1996,  4083,  7337,\n",
       "          2442,  2022,  7775,  2005,  2023,  2896,  5391,  2000,  2907,  1025,\n",
       "          1037,  3928,  7337,  2064,  2022,  1075,  2137,  2820,  1997,  5584,\n",
       "          2997,  1016,  2881,  2008,  2097,  2424,  2028,  1997,  1996,  2659,\n",
       "          1011, 20831, 13782,  1006,  3383,  9061,  2595, 13821,  1006, 28744,\n",
       "          1024, 19410,  1007, 14841,  3726,  3945,  1007,  1010,  1998,  6516,\n",
       "          1996,  2896,  5391,  2006, 20831,  3685,  2907,  1999,  2236,  1012,\n",
       "          5262,  1010,  2057, 21573,  1996,  4083,  7337,  2000,  2022,  2334,\n",
       "          1025,  2043,  1037,  2731,  7099,  2003,  8209,  2046,  1996,  2897,\n",
       "          1010,  2169, 11265, 21017,  2038,  3229,  2069,  2000,  2216,  9017,\n",
       "          3344,  2011,  2993,  1998,  1996, 15698,  2009,  2003,  3495,  4198,\n",
       "          2000,  1012,  2023,  2003,  1037,  2844, 11213,  2008, 23329,  2015,\n",
       "         12138,  4083, 10595,  2109,  1999, 15756,  1011,  2897,  4275,  1010,\n",
       "          2021,  2089,  2022,  2062, 24286,  2013,  1037,  6897,  2391,  1997,\n",
       "          3193,   102])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.with_format(type=\"torch\", columns=[\"ids\"])\n",
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source_id', 'year', 'title', 'abstract', 'text', 'ids'],\n",
       "    num_rows: 9680\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dbf22187c464f18bb5e0a720a7a27a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xr/lfv3bx8j69x4n2wzsd6ryvp40000gn/T/ipykernel_93833/329929304.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(data[\"ids\"]).unsqueeze(0)\n",
      "/var/folders/xr/lfv3bx8j69x4n2wzsd6ryvp40000gn/T/ipykernel_93833/329929304.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(data[\"ids\"]).unsqueeze(0)\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "pad_index = 0\n",
    "\n",
    "\n",
    "def classify(data, model):\n",
    "    \"\"\"Classify text using model and return classification.\"\"\"\n",
    "    input_ids = torch.tensor(data[\"ids\"]).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        cls = outputs.last_hidden_state[0, 0, :]\n",
    "    return {\"cls\": cls}\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    classify, fn_kwargs={\"model\": BertModel.from_pretrained(\"bert-base-uncased\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.with_format(type=\"torch\", columns=[\"cls\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source_id', 'year', 'title', 'abstract', 'text', 'ids', 'cls'],\n",
       "    num_rows: 9680\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_similar(cls_q, dataset, k=10):\n",
    "    \"\"\"Get top k similar papers to query cls.\"\"\"\n",
    "    cls = dataset[\"cls\"]\n",
    "    sim = (cls @ cls_q.T).squeeze()\n",
    "    idx = sim.argsort(descending=True)[:k]\n",
    "    print(sim[idx])\n",
    "    return idx\n",
    "\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([187.6234, 184.4373, 184.4104, 183.4597, 181.1204, 180.9749, 180.8561,\n",
      "        180.8233, 180.6211, 180.0400])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3713, 9499, 4331, 8911, 5948, 8453, 9462, 519, 4707, 6309]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Recurrent Neural Networks\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# Get the tokens' embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "cls_q = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "idx = top_k_similar(cls_q, dataset, k=10)\n",
    "idx = idx.tolist()\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([170.9345, 169.3526, 168.3620, 168.2141, 166.3156, 165.7052, 165.6727,\n",
      "        165.0882, 163.4123, 163.1649])\n",
      "[3713, 273, 9499, 4829, 8497, 9267, 6675, 8826, 5214, 5920]\n"
     ]
    }
   ],
   "source": [
    "text = \"RNN\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# Get the tokens' embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "cls_q = outputs.last_hidden_state[:, 0, :]\n",
    "idx = top_k_similar(cls_q, dataset, k=10)\n",
    "idx = idx.tolist()\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([196.2789, 194.6957, 194.4283, 193.8195, 193.0585, 192.8403, 192.7309,\n",
      "        192.2985, 191.6830, 191.0993])\n",
      "[4331, 8911, 8453, 9499, 3713, 8329, 4707, 9464, 7575, 8930]\n"
     ]
    }
   ],
   "source": [
    "text = \"Recurrent Neural Networks (RNN)\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# Get the tokens' embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "cls_q = outputs.last_hidden_state[:, 0, :]\n",
    "idx = top_k_similar(cls_q, dataset, k=10)\n",
    "idx = idx.tolist()\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select and Sample - A Model of Efficient Neural Inference and Learning\n",
      "An increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli, which is used to act close to optimally in the environment. One outstanding difficulty with this hypothesis is that the exact posterior will in general be too complex to be represented directly, and thus neurons will have to represent an approximation of this distribution. Two influential proposals of efficient posterior representation by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational approximation of the posterior. We show that these approaches can be combined for an inference scheme that retains the advantages of both: it is able to represent multiple modes and arbitrary correlations, a feature of sampling methods, and it reduces the represented space to regions of high probability mass, a strength of variational approximations. Neurally, the combined method can be interpreted as a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of Markov Chain Monte Carlo (MCMC) to approximate the posterior over the relevant states. We demonstrate the effectiveness and efficiency of this approach on a sparse coding model. In numerical experiments on artificial data and image patches, we compare the performance of the algorithms to that of exact EM, variational state space selection alone, MCMC alone, and the combined select and sample approach. The select and sample approach integrates the advantages of the sampling and variational approximations, and forms a robust, neurally plausible, and very efficient model of processing and learning in cortical networks. For sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions.\n",
      "\n",
      "\n",
      "Comparison Against Task Driven Artificial Neural Networks Reveals Functional Properties in Mouse Visual Cortex\n",
      "Partially inspired by features of computation in visual cortex, deep neural networks compute hierarchical representations of their inputs.  While these networks have been highly successful in machine learning, it is still unclear to what extent they can aid our understanding of cortical function.  Several groups have developed metrics that provide a quantitative comparison between representations computed by networks and representations measured in cortex.  At the same time, neuroscience is well into an unprecedented phase of large-scale data collection, as evidenced by projects such as the Allen Brain Observatory.  Despite the magnitude of these efforts, in a given experiment only a fraction of units are recorded, limiting the information available about the cortical representation.  Moreover, only a finite number of stimuli can be shown to an animal over the course of a realistic experiment.  These limitations raise the question of how and whether metrics that compare representations of deep networks are meaningful on these data sets.  Here, we empirically quantify the capabilities and limitations of these metrics due to limited image and neuron sample spaces.  We find that the comparison procedure is robust to different choices of stimuli set and the level of sub-sampling that one might expect in a large scale brain survey with thousands of neurons.  Using these results, we compare the representations measured in the Allen Brain Observatory in response to natural image presentations.  We show that the visual cortical areas are relatively high order representations (in that they map to deeper layers of convolutional neural networks).  Furthermore, we see evidence of a broad, more parallel organization rather than a sequential hierarchy, with the primary area VisP (V1) being lower order relative to the other areas.\n",
      "\n",
      "\n",
      "Turbo Autoencoder: Deep learning based channel codes for point-to-point communication channels\n",
      "Designing codes that combat the noise in a communication medium has remained a significant area of research in information theory as well as wireless communications. Asymptotically optimal channel codes have been developed by mathematicians for communicating under canonical models after over 60 years of research. On the other hand, in many non-canonical channel settings, optimal codes do not exist and the codes designed for canonical models are adapted via heuristics to these channels and are thus not guaranteed to be optimal. In this work, we make significant progress on this problem by designing a fully end-to-end jointly trained neural encoder and decoder, namely, Turbo Autoencoder (TurboAE), with the following contributions: (a) under moderate block lengths, TurboAE approaches state-of-the-art performance under canonical channels; (b) moreover, TurboAE outperforms the state-of-the-art codes under non-canonical settings in terms of reliability. TurboAE shows that the development of channel coding design can be automated via deep learning, with near-optimal performance.\n",
      "\n",
      "\n",
      "Inherent Weight Normalization in Stochastic Neural Networks\n",
      "Multiplicative stochasticity such as Dropout improves the robustness and gener-\n",
      "alizability deep neural networks. Here, we further demonstrate that always-on\n",
      "multiplicative stochasticity combined with simple threshold neurons provide a suf-\n",
      "ficient substrate for deep learning machines. We call such models Neural Sampling Machines (NSM). We find that the probability of activation of the NSM exhibits a self-normalizing property that mirrors Weight Normalization, a previously studied mechanism that fulfills many of the features of Batch Normalization in an online fashion. The normalization of activities during training speeds up convergence by preventing internal covariate shift caused by changes in the distribution of inputs. The always-on stochasticity of the NSM confers the following advantages: the network is identical in the inference and learning phases, making the NSM a suitable substrate for continual learning, it can exploit stochasticity inherent to a physical substrate such as analog non-volatile memories for in memory computing, and it is suitable for Monte Carlo sampling, while requiring almost exclusively addition and comparison operations. We demonstrate NSMs on standard classification benchmarks (MNIST and CIFAR) and event-based classification benchmarks (N-MNIST and DVS Gestures). Our results show that NSMs perform comparably or better than conventional artificial neural networks with the same architecture.\n",
      "\n",
      "\n",
      "Discriminative Network Models of Schizophrenia\n",
      "Schizophrenia is a complex psychiatric disorder that has eluded a characterization in terms of local abnormalities of brain activity, and is hypothesized to affect the collective, ``emergent working of the brain. We propose a novel data-driven approach to capture emergent features using functional brain networks [Eguiluzet al] extracted from fMRI data, and demonstrate its advantage over traditional region-of-interest (ROI) and local, task-specific linear activation analyzes. Our results suggest that schizophrenia is indeed associated with disruption of global, emergent brain properties related to its functioning as a network, which cannot be explained by alteration of local activation patterns. Moreover, further exploitation of interactions by sparse Markov Random Field classifiers shows clear gain over linear methods, such as Gaussian Naive Bayes and SVM, allowing to reach 86% accuracy (over 50% baseline - random guess), which is quite remarkable given that it is based on a single fMRI experiment using a simple auditory task.\n",
      "\n",
      "\n",
      "Adversarial Robustness through Local Linearization\n",
      "Adversarial training is an effective methodology for training deep neural networks that are robust against adversarial, norm-bounded perturbations. However, the computational cost of adversarial training grows prohibitively as the size of the model and number of input dimensions increase. Further, training against less expensive and therefore weaker adversaries produces models that are robust against weak attacks but break down under attacks that are stronger. This is often attributed to the phenomenon of gradient obfuscation; such models have a highly non-linear loss surface in the vicinity of training examples, making it hard for gradient-based attacks to succeed even though adversarial examples still exist. In this work, we introduce a novel regularizer that encourages the loss to behave linearly in the vicinity of the training data, thereby penalizing gradient obfuscation while encouraging robustness. We show via extensive experiments on CIFAR-10 and ImageNet, that models trained with our regularizer avoid gradient obfuscation and can be trained significantly faster than adversarial training. Using this regularizer, we exceed current state of the art and achieve 47% adversarial accuracy for ImageNet with L-infinity norm adversarial perturbations of radius 4/255 under an untargeted, strong, white-box attack. Additionally, we match state of the art results for CIFAR-10 at 8/255.\n",
      "\n",
      "\n",
      "Identification of Recurrent Patterns in the Activation of Brain Networks\n",
      "Identifying patterns from the neuroimaging recordings of brain activity  related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) defining a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series.  In this paper, we present a network-aware feature-space to represent the states of a general network, that enables  comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efficient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting ``mass'' over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efficiency of the approximation, especially for large problems.  While the application presented here is for identifying distinct brain activity patterns from fMRI, this feature-space can be applied to the problem of identifying recurring patterns and detecting outliers in measurements on many different types of networks, including sensor, control and social networks.\n",
      "\n",
      "\n",
      "Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics\n",
      "Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks.\n",
      "\n",
      "\n",
      "A Lyapunov-based Approach to Safe Reinforcement Learning\n",
      "In many real-world reinforcement learning (RL) problems, besides optimizing the main objective function, an agent must concurrently avoid violating a number of constraints. In particular, besides optimizing performance, it is crucial to guarantee the safety of an agent during training as well as deployment (e.g., a robot should avoid taking actions - exploratory or not - which irrevocably harm its hard- ware). To incorporate safety in RL, we derive algorithms under the framework of constrained Markov decision processes (CMDPs), an extension of the standard Markov decision processes (MDPs) augmented with constraints on expected cumulative costs. Our approach hinges on a novel Lyapunov method. We define and present a method for constructing Lyapunov functions, which provide an effective way to guarantee the global safety of a behavior policy during training via a set of local linear constraints. Leveraging these theoretical underpinnings, we show how to use the Lyapunov approach to systematically transform dynamic programming (DP) and RL algorithms into their safe counterparts. To illustrate their effectiveness, we evaluate these algorithms in several CMDP planning and decision-making tasks on a safety benchmark domain. Our results show that our proposed method significantly outperforms existing baselines in balancing constraint satisfaction and performance.\n",
      "\n",
      "\n",
      "Offline Contextual Bayesian Optimization\n",
      "In black-box optimization, an agent repeatedly chooses a configuration to test, so as to find an optimal configuration.\n",
      "In many practical problems of interest, one would like to optimize several systems, or ``tasks'', simultaneously; however, in most of these scenarios the current task is determined by nature. In this work, we explore the ``offline'' case in which one is able to bypass nature and choose the next task to evaluate (e.g. via a simulator). Because some tasks may be easier to optimize and others may be more critical, it is crucial to leverage algorithms that not only consider which configurations to try next, but also which tasks to make evaluations for. In this work, we describe a theoretically grounded Bayesian optimization method to tackle this problem. We also demonstrate that if the model of the reward structure does a poor job of capturing variation in difficulty between tasks, then algorithms that actively pick tasks for evaluation may end up doing more harm than good. Following this, we show how our approach can be used for real world applications in science and engineering, including optimizing tokamak controls for nuclear fusion.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in idx:\n",
    "    print(dataset[\"title\"][i])\n",
    "    print(dataset[\"abstract\"][i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Recurrent Neural Networks (RNN).\"\n",
    "document = \"An abstract from the NeurIPS database that discusses \" + str(\n",
    "    dataset[\"abstract\"][9464]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An abstract from the NeurIPS database that discusses Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks.\n"
     ]
    }
   ],
   "source": [
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minlingzhou/miniconda3/envs/my-env/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Recurrent Neural Networks (RNN). The RNN consists of 3 separate networks: recurrent neural networks (RCTs), recurrent neural networks (RDNs), and recurrent neural populations (RNCs). RCTs are generally considered as being more complex than RDNs due to the nature of the structure and algorithm used to develop them. RNNs allow for relatively simple and cost prohibitive applications (such as learning, memory, and neural network training) and are capable of delivering complex training outputs as long as the required dataset is available. The RNN has four features:\\n\\nUniformity: The dataset will be different when the original data is stored in the same form as a priori. In particular, when the dataset is stored in an RNNs container, when the initial input data is presented as a single row, the entire dataset will be stored in the same form as the original.\\n\\nThe dataset will be different when the original data is stored in the same form as a priori. In particular, when the initial input data is presented as a single row, the entire dataset will be stored in the same form as The original. Decompression: In order to allow for faster and reproducible retrieval, RNNs can be decompressed into arbitrary chunks of data. The output output formats for these chunks of data can be as large as 512 MB on both sides. However, a significant number of RNN projects and individuals use this method. In this paper, we will assume that we can compute these chunks at a high-resolution with a resolution of 12Mb and produce output at resolutions of 40X or more.\\n\\nA set of three RNNs is used:\\n\\nRNN1 - A recurrent neural network with high accuracy but high latency\\n\\nRNN2 - A recurrent neural network with low latency but high efficiency\\n\\nRNN3 - A recurrent neural network with low efficiency but high frequency\\n\\nThe resulting RNNs are implemented using the following RNN:\\n\\nRNN1 - Random number generator: All of the RNNs generated by this project are randomly generated as a subset of other RNNs in the same set.\\n\\nRandom number generator: All of the RNNs generated by this project are randomly generated as a subset of other RNNs in the same set. RNN2 - A recurrent neural network with low accuracy but high latency\\n\\nRNN3 - A recurrent neural network with low latency but high efficiency\\n\\nRNN4 -'},\n",
       " {'generated_text': \"Recurrent Neural Networks (RNN). Now in its third month, RNN offers a way to predict the exact neural activity of a user's hand (using a device that is known to have a good spatial accuracy) and predict the overall activity of the user's hand. It features a large range of features which are designed to target particular devices and applications in different kinds of applications. RNN is also used for the detection of neural-device problems like neural net networks.\\n\\nIn fact RNN is well known and well funded (in US$3 billion by the government). The first RNN software written for smartphones was the RNN software for Android that was produced by Cisco Systems in January of 2014. Since then, numerous different RNN applications have been released. Some of these applications include non-linear (for example, RNN is a distributed computing engine), linearizable, probabilistic and multisort-based applications; non-linear linear network architectures; and non-sequential adaptive architectures (an adaptive algorithm for learning a new sentence).\\n\\nRNN is also often used in the marketing side as a signal processor. One example is the idea of a computerized speech recognition system like Google Translate for commercial use. Another example is the neural network used by the Stanford University researchers in a clinical trial.\\n\\nThe RNN system, as mentioned above, is an adaptation of the classical statistical problem classification algorithm. For instance in this case an English speaker might be trained to identify a person. Similar to in many other languages, RNN is designed to make use of specific brain regions and the associated information. The underlying idea is that RNN is used by the researchers to predict behavior. For instance, the researchers create an algorithm to predict brain activity in a particular condition for a particular condition (given a set of conditions). The researchers use the information in any condition and have made the prediction for it.\\n\\nThe researchers also create a software API that allows developers to create their own RNN modules within a framework which includes RNN. RNN module is essentially one RNN module which creates an RNN that runs within the RNN module.\\n\\nWhy is data storage hard to write?\\n\\nIn some cases, data storage is not available, and data management systems such as SSDs or datacenters are expensive and slow. In addition, storage systems cannot support large amounts of data or can't handle a high number of data requests. Data has to be stored on the computer. In general, data storage is\"},\n",
       " {'generated_text': \"Recurrent Neural Networks (RNN). This has already been discussed in detail here. The first version of a recurrent neural network using KANNN is shown above and the second version of a RNN using TIBN is shown here. R-NNs (also call N-NNs) work by replacing the two main signals of an input signal with a non-functional version. The N-NN is an alternative to the traditional RNN in that it is based on the current set of N-NNs. Unlike the previous versions, N-NN-like features are not shown in the picture here. The other NNN version in this series uses only N-NNs to generate a new one. This was also reported the same weekend in a paper on the Neural Networks of People (W.C.T.) by Pekka Jekki.\\n\\nFor this experiment, we are going to add a series of NNNs to the current sets of models and see how well they fit together. We do this by working with the existing sets of models and by learning new ones. For example, imagine the NNNs in the same set of models, each representing a different set of neurons. The N-Neutral networks are like a list, and the N-Nearest pairs are N. A common task in our experiments is to train the same set of N-Neutral networks over time. Each neuron produces a new set of N-Neutral networks each time, and the rest of the N-Neutral networks are the same. These groups are similar in function and size, as long as one can combine the results of all the previous NNNs together!\\n\\nOne big note to note with these N-Neutral networks: they are highly specific of a set of neurons that is present in all of the neural networks available for learning to model the world without using the same set of neurons. That's why they are unique in that they are only used for training these N-Neutral networks. This makes building the network from data generated by N-Neutral NNNs a lot easier and can help build more N-Neutral networks more quickly. The problem is that you generally end up with more N-Neutral networks per neuron than a previous dataset, so a new set of N-Neutral NNNs can fit the previous dataset too. That sounds like a really big problem, but it's an easy one.\\n\\nLet's explore how these N-Neutral networks\"},\n",
       " {'generated_text': 'Recurrent Neural Networks (RNN).'},\n",
       " {'generated_text': \"Recurrent Neural Networks (RNN). The RNN was designed for deep exploration as both a learning framework or a computational model. In this article, I look at a few of the models and describe the neural networks used in this paper.\\n\\nBackground\\n\\nAs of April 2014, the Neural Network Programming Language (NNLP) has been developed with the intention of teaching some of my students a way to build scalable RNNs. Despite the many iterations and problems presented to me over the years, I found the NNNLP has one simple point of pride (I could tell you which model and model is correct).\\n\\nMost RNNs are built as discrete neural networks, or CNNs, within a single layer layer of an unstructured network. These CNNs have high performance in memory, and they are extremely fast. For that reason, this article introduces a particular approach to building a CNN with RNN-based techniques.\\n\\nThe CNN model\\n\\nThis is the simplest implementation I could find for this paper (the paper is in C# and the corresponding example below is part of the C# Core Library).\\n\\nThis is an ordinary linear RNN, which produces a matrix with the value of c(x) as the value of b. It is the model:\\n\\nA simple RNN with low accuracy, but high performance in memory.\\n\\nCaffe's (Caffe)\\n\\nCaffe's is a RNN that was designed to be able to quickly solve any problem (with a low memory cost, higher computing power, and high learning time). It employs a combination of regularization of the matrix (nearest neighbors) and regularization of the inputs, and its random distribution function is used to define any number of possible values depending on the input, such as in the standard input.\\n\\nNote that Caffe is still working on making more effective discriminants for C# as the algorithm evolves to support C# 5.0 for performance purposes.\\n\\nA typical Caffe L2D is written so that it can be easily read directly within C++:\\n\\nstd::map<string, int, int> mapS(uint8_t &value,...) { uint8_t val = value; // initializations return new uint8_t (val.t) * value; // initializations return new void(*val); }\\n\\nTo compile our Caffe L1D (with the code above) this means that the\"}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "set_seed(42)\n",
    "generator(prompt, max_length=512, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Recurrent Neural Networks (RNN). An abstract from the NeurIPS database that discusses Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks. We note that several of the above properties of recurrent neural networks were not fully elucidated by previous studies (see also Semen et al. 2015 ), and that despite some improvements, such as automatic processing at lower threshold, generalizability is still often not demonstrated. Hence, the above references are incomplete, and most cannot be considered definitive.\\n\\nAcknowledgments We thank Semen, Gao, Hoe, and Youssawi for their help with the study. We also thank Daniel Kuyma for his help on several drafts of this manuscript. We also thank James F. for helping out in writing this paper. All authors are anonymous due to restrictions concerning their specific capacity to speak and communicate.'},\n",
       " {'generated_text': \"Recurrent Neural Networks (RNN). An abstract from the NeurIPS database that discusses Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks.\\n\\nAcknowledgments The contribution of G.C. was made by G.S., M.I. (F.M.) and C.D. to support the statistical analyses for which the original author is indebted. G.C.'s contribution is also supported by a fellowship that is currently on loan to the Stanford Program for Computer-Aided Systems Analysis (SACA) at Stanford since 1999 and the IEEE Computer Society's Award of The RNN Award at The University of Pennsylvania (now Open Computer). The author's training was supported by a variety of contributions, including S.F., D.P., and E.J.C. in preparation for the postdoctoral research on K.P., L., J., B.E., and M.H.S., J. D.P., N.R.-U.S., and S.F. in preparation for postdoc work. The authors have no conflict of interest/career affiliation with any fund of the Stanford University School of Computer Medicine\"},\n",
       " {'generated_text': 'Recurrent Neural Networks (RNN). An abstract from the NeurIPS database that discusses Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks. Such systems are typically designed to enable robust statistical inference, to analyze a sequence of spatially continuous and time-varying stimuli, and to optimize memory for the most recent neural network iteration in order to learn the next iteration of a given sentence. However, this mechanism might be especially useful in situations where complex computational tools do not permit the task to be understood in a general, systematic way; for instance, in situations where complex processes are common in the past and require a large ensemble of neurons to compute each individual sentence, our findings indicate a possible alternative method for doing so. Such training schemes, when applied to recurrent neural networks, enable the analysis of tensor streams, or spatially continuous sequences, of input data, and to perform a set of computational inference operations on a sequence of spatially continuous stimuli (see fig. 15). To test this idea above, we first trained recurrent network (RNN) models in the real-time time domain and in an image-oriented supervised learning model, performing a 2  2 RM'},\n",
       " {'generated_text': 'Recurrent Neural Networks (RNN). An abstract from the NeurIPS database that discusses Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks.'},\n",
       " {'generated_text': 'Recurrent Neural Networks (RNN). An abstract from the NeurIPS database that discusses Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks.'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "set_seed(42)\n",
    "generator(prompt + \" \" + document, max_length=512, num_return_sequences=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
