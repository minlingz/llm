{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "## Student: Minling Zhou\n",
    "## NetID: mz246\n",
    "\n",
    "We are interested in sentiment analysis. Given a short document, we wish to assess whether the corresponding sentiment is positive (label 𝑦 = 1 ) or negative (label 𝑦 = 0)\n",
    "We will do this with the following prescription:\n",
    "1. For every word, we will learn a corresponding 𝑑-dimensional vector, i.e., $𝑥_i$ ∈ 𝑅\" for word\n",
    "𝑖 in the vocabulary. If we have a total of 𝑁 words in the vocabulary, we learn 𝑥! , for 𝑖 = 1, ... , 𝑁 \n",
    "2. Assume that there are $𝑀_j$ words in document 𝑗. The feature vector for this document is\n",
    "\\begin{equation}\n",
    "𝑓_j = \\frac{1}{𝑀_j}\\sum_{m=1}^{M_j}x_{w(m,j)}\n",
    "\\end{equation}\n",
    "where $x_{w(m,j)}$ is the vector associated with the 𝑚th word in document 𝑗. \n",
    "3. The probability of positive sentiment for document 𝑗 is modeled as\n",
    "\\begin{equation}\n",
    "p(Y=1|f_j) = \\sigma[w \\cdot f_j + b]\n",
    "\\end{equation}\n",
    ".\n",
    "\n",
    "The unknown parameters, which must be learned based on data, are $𝑥_i$ , corresponding to the vectors for each of the 𝑁 words in the total vocabulary, as well as the weight vector 𝑤 ∈ 𝑅\" and bias 𝑏\n",
    "\n",
    "Show the detailed derivation of the cost function for this formulation, and provide a detailed derivation of the gradients to be used in gradient descent. In your writeup, show all details of the implementation.\n",
    "\n",
    "Assume that the learning will be performed with 𝐾 documents, and 𝐾 is very large. Show in detail how you will scale the model to handle 𝐾 that is too large for all documents to be handled at once on a computer.\n",
    "\n",
    "Write code in Python, to implement this model, and use Adam as the method to implement gradient descent. Use\n",
    "the following dataset: https://huggingface.co/datasets/yelp_polarity\n",
    "\n",
    "In your solution, provide the code, and also provide a detailed analysis on the accuracy of the predictions on test data (complete report of results). Explain carefully how you constituted the training, validation and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "### Cost Function\n",
    "Logistic regression uses a sigmoid function to map predicted values to probabilities. The cost function, also known as the log loss, measures the performance of the model. It is defined as:\n",
    "\\begin{equation}\n",
    "J(w,b) = -\\frac{1}{K}\\sum_{i=1}^{K}y_i\\log(p(Y=1|f_i)) + (1-y_i)\\log(1-p(Y=1|f_i))\n",
    "\\end{equation}\n",
    "where $K$ is the number of documents, $y_i$ is the true label of document $i$, and $p(Y=1|f_i)$ is the predicted probability of document $i$ being positive. The predicted probability is given by the sigmoid function:\n",
    "\\begin{equation}\n",
    "p(Y=1|f_i) = \\sigma[w \\cdot f_i + b]\n",
    "\\end{equation}\n",
    "where $\\sigma$ is the sigmoid function, $w$ is the weight vector, and $b$ is the bias.\n",
    "\n",
    "### Derivation of the Gradients\n",
    "The gradients of the cost function with respect to the weight vector and bias are used to update the parameters during training. The gradients are given by:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{K}\\sum_{i=1}^{K}(p(Y=1|f_i)-y_i)f_i\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{K}\\sum_{i=1}^{K}(p(Y=1|f_i)-y_i)\n",
    "\\end{equation}\n",
    "where $K$ is the number of documents, $y_i$ is the true label of document $i$, and $p(Y=1|f_i)$ is the predicted probability of document $i$ being positive.\n",
    "\n",
    "### Gradient Descent Update Rule\n",
    "The parameters are updated using the gradients and the learning rate. The update rule for the weight vector and bias is given by:\n",
    "\\begin{equation}\n",
    "w = w - \\alpha\\frac{\\partial J(w,b)}{\\partial w}\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "b = b - \\alpha\\frac{\\partial J(w,b)}{\\partial b}\n",
    "\\end{equation}\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "### Implementation Steps\n",
    "* `Initialize the weight vector and bias`: Initialize the weight vector and bias to small random values.\n",
    "* `Compute the Prediction`: For each document, compute the predicted probability using the sigmoid function.\n",
    "* `Compute the Cost`: Compute the cost function using the predicted probabilities and true labels.\n",
    "* `Compute the Gradients`: Compute the gradients of the cost function with respect to the weight vector and bias.\n",
    "* `Update the Parameters`: Update the weight vector and bias using the gradients and the learning rate.\n",
    "* `Repeat until convergence`: Repeat steps 2-5 until the cost function converges or a maximum number of iterations is reached.\n",
    "\n",
    "\n",
    "### Scaling the Model\n",
    "When the number of documents is very large, it is not feasible to handle all documents at once on a computer. To handle large datasets, there are quite a few options:\n",
    "* `Mini-batch Gradient Descent`: Divide the dataset into mini-batches and update the parameters using the gradients computed on each mini-batch.\n",
    "* `Stochastic Gradient Descent (SGD)`: Instead of using the entire dataset to compute the gradient of the cost function, SGD uses a single or a small sample of the data. This makes the computation much faster and scalable for large datasets.\n",
    "* `Algorithmic Efficiency`: Use advanced optimization algorithms like Adam or RMSprop, which are more efficient than standard gradient descent, especially for large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_polarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {split: dataset.to_pandas() for split, dataset in dataset.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559995</th>\n",
       "      <td>Ryan was as good as everyone on yelp has claim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559996</th>\n",
       "      <td>Professional \\nFriendly\\nOn time AND affordabl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559997</th>\n",
       "      <td>Phone calls always go to voicemail and message...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559998</th>\n",
       "      <td>Looks like all of the good reviews have gone t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559999</th>\n",
       "      <td>Ryan Rocks! I called him this morning for some...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0       Unfortunately, the frustration of being Dr. Go...      0\n",
       "1       Been going to Dr. Goldberg for over 10 years. ...      1\n",
       "2       I don't know what Dr. Goldberg was like before...      0\n",
       "3       I'm writing this review to give you a heads up...      0\n",
       "4       All the food is great here. But the best thing...      1\n",
       "...                                                   ...    ...\n",
       "559995  Ryan was as good as everyone on yelp has claim...      1\n",
       "559996  Professional \\nFriendly\\nOn time AND affordabl...      1\n",
       "559997  Phone calls always go to voicemail and message...      0\n",
       "559998  Looks like all of the good reviews have gone t...      0\n",
       "559999  Ryan Rocks! I called him this morning for some...      1\n",
       "\n",
       "[560000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "n = 560000\n",
    "\n",
    "sentences = df[\"train\"][\"text\"].values.tolist()[:n]\n",
    "y = df[\"train\"][\"label\"].values.tolist()[:n]\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Create and train the Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4\n",
    ")\n",
    "\n",
    "vectorized_sentences = [model.wv[s].mean(axis=0) for s in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.9000133928571429\n",
      "test: 0.8998839285714286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    vectorized_sentences, y, test_size=0.2, random_state=0\n",
    ")\n",
    "clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
    "print(\"train:\", clf.score(X_train, y_train))\n",
    "print(\"test:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 89.0\n",
      "test: 90.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    torch.Tensor(vectorized_sentences),\n",
    "    torch.Tensor(y).unsqueeze(1),\n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 5000  # You can change this value\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Define the logistic regression model\n",
    "class LogisticRegressionPyTorch(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(LogisticRegressionPyTorch, self).__init__()\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegressionPyTorch(X_train.shape[1])\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 8\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "def binary_accuracy(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(y_pred)\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum / y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc\n",
    "\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_acc = binary_accuracy(model(X_train), y_train)\n",
    "    test_acc = binary_accuracy(model(X_test), y_test)\n",
    "\n",
    "print(\"train:\", train_acc.item())\n",
    "print(\"test:\", test_acc.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
